---
title: "Online News Popularity - R Project"
output: html_document
Authors: Benjamin Ardila, Sebastian Van Houdt, Sondos Atwi
---

The purpose of this project is to use regression to predict the number of shares of articles by mashable. 

Dataset source: http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity

abstract: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity).
Data Set Characteristics:  Multivariate
Number of Instances: 39797
Area: Business

Attribute Characteristics:Integer, Real

Number of Attributes:61

Date Donated: 2015-05-31

Data Set Information:

* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls. 
* Acquisition date: January 8, 2015 
* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.


Attribute Information:
Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field). 

Attribute Information:
0. url: URL of the article (non-predictive) -- not to be included in the analysis
1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) -- not to be included in the analysis
2. n_tokens_title: Number of words in the title 
3. n_tokens_content: Number of words in the content 
4. n_unique_tokens: Rate of unique words in the content 
5. n_non_stop_words: Rate of non-stop words in the content 
Examples of stop words: 
Determiners- Determiners tend to mark nouns where a determiner usually will be followed by a noun
examples: the, a, an, another
Coordinating conjunctionsâ€“ Coordinating conjunctions connect words, phrases, and clauses
examples: for, an, nor, but, or, yet, so
Prepositions- Prepositions express temporal or spatial relations
examples: in, under, towards, before
	
6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content 

7. num_hrefs: Number of links 

8. num_self_hrefs: Number of links to other articles published by Mashable 

9. num_imgs: Number of images
 
10. num_videos: Number of videos

11. average_token_length: Average length of the words in the content

12. num_keywords: Number of keywords in the metadata 

13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?
14. data_channel_is_entertainment: Is data channel 'Entertainment'?
15. data_channel_is_bus: Is data channel 'Business'?
16. data_channel_is_socmed: Is data channel 'Social Media'?
17. data_channel_is_tech: Is data channel 'Tech'?
18. data_channel_is_world: Is data channel 'World'?

19. kw_min_min: Worst keyword (min. shares)
20. kw_max_min: Worst keyword (max. shares)
21. kw_avg_min: Worst keyword (avg. shares)
22. kw_min_max: Best keyword (min. shares)
23. kw_max_max: Best keyword (max. shares)
24. kw_avg_max: Best keyword (avg. shares)
25. kw_min_avg: Avg. keyword (min. shares)
26. kw_max_avg: Avg. keyword (max. shares)
27. kw_avg_avg: Avg. keyword (avg. shares)

28. self_reference_min_shares: Min. shares of referenced articles in Mashable
29. self_reference_max_shares: Max. shares of referenced articles in Mashable
30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable

31. weekday_is_monday: Was the article published on a Monday?
32. weekday_is_tuesday: Was the article published on a Tuesday?
33. weekday_is_wednesday: Was the article published on a Wednesday?
34. weekday_is_thursday: Was the article published on a Thursday?
35. weekday_is_friday: Was the article published on a Friday?
36. weekday_is_saturday: Was the article published on a Saturday?
37. weekday_is_sunday: Was the article published on a Sunday?
38. is_weekend: Was the article published on the weekend?

39. LDA_00: Closeness to LDA topic 0
40. LDA_01: Closeness to LDA topic 1
41. LDA_02: Closeness to LDA topic 2
42. LDA_03: Closeness to LDA topic 3
43. LDA_04: Closeness to LDA topic 4

44. global_subjectivity: Text subjectivity 
45. global_sentiment_polarity: Text sentiment polarity
46. global_rate_positive_words: Rate of positive words in the content 
47. global_rate_negative_words: Rate of negative words in the content
48. rate_positive_words: Rate of positive words among non-neutral tokens
49. rate_negative_words: Rate of negative words among non-neutral tokens

50. avg_positive_polarity: Avg. polarity of positive words
51. min_positive_polarity: Min. polarity of positive words
52. max_positive_polarity: Max. polarity of positive words
53. avg_negative_polarity: Avg. polarity of negative words
54. min_negative_polarity: Min. polarity of negative words
55. max_negative_polarity: Max. polarity of negative words

56. title_subjectivity: Title subjectivity
57. title_sentiment_polarity: Title polarity
58. abs_title_subjectivity: Absolute subjectivity level
59. abs_title_sentiment_polarity: Absolute polarity level

60. shares: Number of shares (target)

We have based some of our analysis on the paper "Predicting and Evaluating the Popularity of Online News" that can be found here:
http://cs229.stanford.edu/proj2015/328_report.pdf

###Start by reading the data and have a look at its structure:
```{r}
library(dplyr)
library(ggplot2)
library(GGally)

data = read.csv("OnlineNewsPopularity.csv")
str(data)

set.seed(27)
#Create a sample of the data to reduce the time of processing it
dsmall <- sample_n(data, 10000)


```

#Exploratory Analysis of Variables

First, generate a global summary of the dataset to understand how the values are distributed 

```{r}
summary(dsmall)

```

The main finding of this preliminary analysis is that there is a big difference between ther 3rd Quartile (2700) of the number of shares and its maximum (652900). Having some extreme values for the number of shares will most probably generate a negative effect on the quality of the predictions that we will make.

As a second step of the exploratory analysis, we generated histograms to understand the way different variables are distributed. We didn't perform the analysis for binary variables because histograms don't add much value to understanding their distribution
```{r}

hist(dsmall$ n_tokens_title)
hist(dsmall$ n_tokens_content)
hist(dsmall$ n_unique_tokens)
hist(dsmall$ n_non_stop_words)
hist(dsmall$ n_non_stop_unique_tokens)
hist(dsmall$ num_hrefs)
hist(dsmall$ num_self_hrefs)
hist(dsmall$ num_imgs)
hist(dsmall$ num_videos)
hist(dsmall$ average_token_length)
hist(dsmall$ num_keywords)
hist(dsmall$ kw_min_min)
hist(dsmall$ kw_max_min)
hist(dsmall$ kw_avg_min)
hist(dsmall$ kw_min_max)
hist(dsmall$ kw_max_max)
hist(dsmall$ kw_avg_max)
hist(dsmall$ kw_min_avg)
hist(dsmall$ kw_max_avg)
hist(dsmall$ kw_avg_avg)
hist(dsmall$ self_reference_min_shares)
hist(dsmall$ self_reference_max_shares)
hist(dsmall$ self_reference_avg_sharess)
hist(dsmall$ LDA_00)
hist(dsmall$ LDA_01)
hist(dsmall$ LDA_02)
hist(dsmall$ LDA_03)
hist(dsmall$ LDA_04)
hist(dsmall$ global_subjectivity)
hist(dsmall$ global_sentiment_polarity)
hist(dsmall$ global_rate_positive_words)
hist(dsmall$ global_rate_negative_words)
hist(dsmall$ rate_positive_words)
hist(dsmall$ rate_negative_words)
hist(dsmall$ avg_positive_polarity)
hist(dsmall$ min_positive_polarity)
hist(dsmall$ max_positive_polarity)
hist(dsmall$ avg_negative_polarity)
hist(dsmall$ min_negative_polarity)
hist(dsmall$ max_negative_polarity)
hist(dsmall$ title_subjectivity)
hist(dsmall$ title_sentiment_polarity)
hist(dsmall$ abs_title_subjectivity)
hist(dsmall$ abs_title_sentiment_polarity)
qplot(shares, data=dsmall, geom="histogram",xlim=c(0,40000),ylim=c(0,1500))

```


As a main conclusion, we confirm that the number of shares is mainly very low, but that there are very few exceptions that have a very high number of shares. 


#Identify Regression Variables
Since we have a large number of variables, we will start by creating different variable categories and perform regression analysis on each block. We are doing this to reduce the computational time of running the analysis with a very large number of variables and also as a way to understand better the type of variables the dataset has.

We will pick the best variables of each block and take them into the final pool of variables for running a global model.

Example: Create a Words category that includes all variables related to words: tokens title, tokens content, non-stop words..

Here we will create datasets for all the categories with the variables:

```{r}

words = dsmall %>%
  select(n_tokens_title, n_tokens_content, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens, average_token_length, shares)

links = dsmall %>%  select( num_hrefs, num_self_hrefs,self_reference_min_shares,
 self_reference_max_shares, self_reference_avg_sharess, shares)

media = dsmall %>%
  select(num_imgs,num_videos,shares)

time = dsmall %>%
  select( weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday, is_weekend, shares)

keywords = dsmall %>%
  select( num_keywords, data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world, kw_min_min, kw_max_min, kw_avg_min, kw_min_max, kw_max_max, kw_avg_max, kw_min_avg, kw_max_avg, kw_avg_avg, shares)
  
nlp = dsmall %>%
  select( LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity, avg_negative_polarity, min_negative_polarity, max_negative_polarity, title_subjectivity, title_sentiment_polarity, abs_title_subjectivity, shares)
```

We now have 5 categories: Words, Links, Media, Time, Keywords and NLP (for natural language processing).

We will now start by running the regression method on each of the categories in order to determine the most significant variables and discard the least significant ones

We will be using a maximum tolerable p-value of 5% to select the variables that we would like to keep for building the global model. 

###Block: Words
```{r}
m1 = lm(data = words, shares ~ .)
summary(m1)

#Remove the variables with a p-value > 0.05 and re-run the model
summary(lm(data=words, shares~n_unique_tokens+ n_non_stop_unique_tokens+ average_token_length+ n_tokens_content ))
```

##### We will be keeping the following variables for the model: n_unique_tokens, n_non_stop_unique_tokens, average_token_length, n_tokens_content 

###Block: Links
```{r}
m2 = lm(data = links, shares ~ .)
summary(m2)

#Again, remove the variable with p-value > 0.05 and re-run the modelw it
summary(lm(data = links, shares ~ num_hrefs + num_self_hrefs +self_reference_min_shares))
```

##### We will keep num_hrefs, num_self_hrefs, self_reference_min_shares with p-value < 0.05 for the model

##Block: Media
```{r}
m3 = lm(data = media, shares ~ .)
summary(m3)
```

####Both variables have a p-value < 0.05, so we will keep them both.

###Block: Time
```{r}
library(MASS)

m4 = lm(data = time, shares ~ .)
summary(m4)

#since we have a lot of variables that are not significant, we will run the step-wise function to identify the most significant variables.

null= lm(data=time, shares ~ 1)  
full = lm(data=time, shares ~ .) 

step = stepAIC(null, scope=list(lower=null, upper=full), direction = "forward")

#We will run the model again with the chosen variables
summary(lm(data = time , shares ~ is_weekend + weekday_is_monday ))
#This model shows a high significance for is_weekend, so we will drop weekday_is_monday
detach("package:MASS")
```

###Block: Keywords
```{r}
m5 = lm(data = keywords, shares ~ .)
summary(m5)
```

####Since we have a lot of variables in this category, we will try to divide it further into two categories: type and keys

```{r}
type = dsmall %>%
  select(data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world, shares)

m5 = lm(data = type, shares ~ .)
summary(m5)
```

####From m5, we can see that all the variables now have a p-value < 0.05. So we will not discard any of them.


```{r}
keys = dsmall %>%
  select(kw_min_min, kw_max_min, kw_avg_min, kw_min_max, kw_max_max, kw_avg_max, kw_min_avg, kw_max_avg, kw_avg_avg, shares)

m6 = lm(data = keys, shares ~ .)
summary(m6)
```

####From m6, we find that the significant variables are: kw_max_min, kw_avg_min, kw_max_avg, kw_avg_avg

####We will run a model with these variables and see if we can reduce them further. 
```{r}
summary(lm(data = keys, shares ~ kw_max_min+ kw_avg_min+ kw_max_avg+ kw_avg_avg))

#Rerun the model by removing the variables with p-value > 0.05
summary(lm(data = keys, shares ~ kw_max_avg+ kw_avg_avg))
```
From the model above, we will keep kw_max_avg and kw_avg_avg.


###Block: NLP (Natural language processing)
```{r}
m7 = lm(data = nlp, shares ~ .)
summary(m7)

#Since in this category we also have a lot of variable, we will divide it further into sub categories:
#Create a sub-category 1 for LDA:
lda = dsmall %>%
  select(LDA_00, LDA_01, LDA_02, LDA_03, LDA_04, shares)
  
m8 = lm(data = lda, shares ~ .)
summary(m8)
#Since we have NA in the results, we will try to remove LDA_00, LDA_01 (with the highest p-value) and rerun
lda = dsmall %>%
  select(LDA_02, LDA_03, LDA_04, shares)
 
m9 = lm(data = lda, shares ~ .)
summary(m9)
```

We will try to run a model with only LDA_02 and LDA_03 to see if both are really significant
```{r}
summary(lm(data = lda, shares~ LDA_02+ LDA_03))
```
From the results, we will keep both LDA_02 and LDA_03.


```{r}
#Sub-category 2 measuring polarity
pol = dsmall %>%
  select(global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity, avg_negative_polarity, min_negative_polarity, max_negative_polarity, title_subjectivity, title_sentiment_polarity, abs_title_subjectivity, shares)

m10 = lm(data = pol, shares ~ .)
summary(m10)
#Again, run the model with the variables that have p-value < 0.05.

pol2 = dsmall %>%
  select(global_subjectivity, rate_positive_words, rate_negative_words,  avg_negative_polarity, title_sentiment_polarity, abs_title_subjectivity, shares)

m11 = lm(data = pol2, shares ~ .)
summary(m11)
```

For this second part, we'll keep global_subjectivity, rate_positive_words, rate_negative_words,  avg_negative_polarity, title_sentiment_polarity, abs_title_subjectivity, shares.


#Build Global Regression Model
We will use the step-wise function to identify the most significant variables out of the ones resulting from the previous "block" analysis
```{r}

#Create a new dataset with the variables that we have selected
regData = dsmall %>%
  select(n_unique_tokens, n_non_stop_unique_tokens, average_token_length, n_tokens_content, num_hrefs, num_self_hrefs, self_reference_min_shares, num_imgs, num_videos, is_weekend , kw_max_avg, kw_avg_avg, LDA_03, LDA_02, global_subjectivity, rate_positive_words, rate_negative_words,  avg_negative_polarity, title_sentiment_polarity, abs_title_subjectivity, shares)

library(MASS)
null= lm(data=regData, shares ~ 1)  
full = lm(data=regData, shares ~ .)

step = stepAIC(null, scope=list(lower=null, upper=full), direction = "forward")

summary(step)
detach("package:MASS")
```

Since we have a very low R-squared / Adjusted R-Squared, we will try to visualize the standardized residuals:

```{r}
qplot(predict(step), rstandard(step), geom="point", xlim = c(0,10000)) + geom_hline(yintercept=0, colour=I("blue"), alpha=I(0.5))
```

```{r}

#We will try to remove the variables with the highest p-value and rerun 
mod = lm(formula = shares ~ self_reference_min_shares + kw_avg_avg + 
    kw_max_avg + global_subjectivity + average_token_length + 
    num_hrefs + n_unique_tokens + abs_title_subjectivity + title_sentiment_polarity,
    data = regData)
summary(mod)

#Again, remove the variables with a high p-value
summary(lm(formula = shares ~ self_reference_min_shares + kw_avg_avg + 
    kw_max_avg + global_subjectivity + average_token_length + 
    num_hrefs + n_unique_tokens + abs_title_subjectivity, data = regData))
```


Our final model is the following: 
```{r}
model = lm(formula = shares ~ self_reference_min_shares + kw_avg_avg + 
    kw_max_avg + global_subjectivity + average_token_length + 
    num_hrefs + n_unique_tokens, data = regData)
summary(model)
```


#Model Accuracy

Since we had a low R-Squared, we decide to calculate the accuracy of our model to see how well it predicts the number of shares.

In order to measure the accuracy of the model, and taking into consideration the article "Predicting and Evaluating the Popularity of Online News", we decided to use the Accuracy indicator that can be found in the table IV of the article.

We will split the variable "shares" into 2 groups of equal size and generate a prediction that will also be split into 2 groups of equal size. Then, the logic of the indicador is to measure how many of the predictions is correct (sum of the number of true positives and true negatives compared to the size of the whole sample).  

```{r}
#Here, we switch to the whole dataset instead of the sample dataset.
summary(data$shares)

#We will add a column aboveMedian to classify the data according to the number of shares with respect to the median in the available dataset. 
#We will switch to work with the whole dataset
data = data %>%
  mutate(aboveMedian = ifelse(shares>= 1400, 1, -1))

fitted.results = predict(model,data,type='response')
#expected median
med = median(fitted.results)
fitted.results = ifelse(fitted.results >= med,1,-1)

misClasificError = mean(fitted.results != data$aboveMedian )
print(paste('Accuracy',1-misClasificError))
```



#Check Model Assumptions

### A1: Checking Linear Relation

```{r linearity2}
qplot(predict(model), rstandard(model), geom="point", xlim= c(0,10000), ylim = c(-1,20)) + geom_hline(yintercept=0, colour=I("blue"), alpha=I(0.5))
```


Residuals are not distributed in a symetrical way around the y-axis suggesting there is no clear linear relation between our predictor and the number of shares. There is a very evident tendency of the errors of being spreaded out more above the x-axis than below it. 

This is related with the fact that the number shares is not evenly distributed as there are very few cases in which the number of shares is extremely high.  

### A2: Checking Normality

We check normality plotting a QQ-plot of the residual as well as a histogram of
the residuals.

```{r normality}
# Check normality using histogram
q1 = qplot(rstandard(model), geom="blank", xlim = c(-1,10)) +
  geom_histogram(aes(y=..density..), colour=I("gray"), binwidth=0.2)+
  stat_function(fun=dnorm, args=list(mean=0, sd=1),
                colour=I("red"), alpha=I(0.5))
# Check normality using qqplot
q2 = qplot(sample=rstandard(model)) +
  geom_abline(slope=1,intercept=0)

library(gridExtra)
grid.arrange(q1, q2, nrow=1)
```

Residuals don't seem to be normally distributed. The distribution of residuals is asymetrical and, even if it's expected value is 0, there is a clear case of skewness. 

### A3: Checking Homoscedasticity

```{r homoscedasticity}
qplot(predict(model), rstandard(model), geom="point", ylim = c(-10,20)) + geom_hline(yintercept=0) +
  geom_hline(yintercept=2, colour = I("red"), alpha=I(0.5)) +
  geom_hline(yintercept=-2, colour = I("red"), alpha=I(0.5))
```

The homoscedasticity assumption is broken as the chart shows a significant amount of points outside of 2-sided standard deviations range of the graph.

Therefore,  we will try to find the optimal transformation for the "y"" variable (shares) in order to obtain a homoscedastic model. 

```{r}
library(car)
spreadLevelPlot(model) 
# the suggested transformation is -0.3020725 
```

We will try to use the transformed variable and see if the model improves
```{r}
regData = regData %>%
  mutate(sharesPower = shares^-0.3020725)

model2 = lm(data = regData, formula = sharesPower ~ self_reference_min_shares + kw_avg_avg + 
    kw_max_avg + global_subjectivity + average_token_length + 
    num_hrefs + n_unique_tokens)

summary(model2)
```
We have a slightly higher R-squared. We will continue using this model in the next stages.


```{r homoscedasticity with new model}
qplot(predict(model2), rstandard(model2), geom="point", ylim = c(-10,20)) + geom_hline(yintercept=0) +
  geom_hline(yintercept=2, colour = I("red"), alpha=I(0.5)) +
  geom_hline(yintercept=-2, colour = I("red"), alpha=I(0.5))
```

We can see that with the new model, the homocedasticity assumption is respected.

### A4: Checking Independence

The residuals might have been auto-correlated in the first years of Mashable (founded in 2005) since a very successful article might have led to a higher popularity of the website and therefore it could have influenced the popularity of future articles.

Hence, we want to check if the website is so popular, that it can be considered that there is no time-series effect of the increase in popularity of the website that depends on the number of shares of its articles.

We continue using model 2 to check the rest of the assumptions and run the durbinWatson test to check the independence of the residuals.

```{r independence}
library(car)
durbinWatsonTest(model2)
```
We have a high p-value: the residuals are not auto-correlated. 


# Model Validation

We performed a final validation of the model in order to calculate its performance comparing the training set and a testing set.

We first split the dataset:
```{r}
library(caTools)
set.seed(17)

data = data %>%
  mutate(sharesPower = shares^-0.3020725)

split = sample.split(data$sharesPower, SplitRatio = 0.8) 
training = subset(data, split==TRUE)
testing = subset(data, split==FALSE)

```

We then re-calculate the model with the new training database.  

```{r training}
fit <- lm(data= training, sharesPower ~ self_reference_min_shares + kw_avg_avg + 
    kw_max_avg + global_subjectivity + average_token_length + 
    num_hrefs + n_unique_tokens)

summary(fit)

#We remove the average_token_lenght from the model because it has a very high p-value.

fit <- lm(data= training, sharesPower ~ self_reference_min_shares + kw_avg_avg + 
    kw_max_avg + global_subjectivity + average_token_length + 
    num_hrefs)

summary(fit)

Rsq.Training = summary(fit)$r.squared 

```

And we finally evaluate the R-Squared for the testing dataset

```{r testing}
SSE = (predict(fit, newdata = testing) - testing$sharesPower)^2 %>% sum()
SSTotal = (testing$sharesPower - mean(testing$sharesPower))^2 %>% sum()
Rsq.Testing = (SSTotal - SSE)/SSTotal
cat("R^2 Training = ", Rsq.Training, " vs R^2 Testing ", Rsq.Testing, ".", sep="")
```

# Final Considerations

Even if we found a final model that has a set of variables with very low p-values (lower than 10e-15), the final outcome is a model with an R-Squared barely above zero.

The predicted variable has a very particular distribution that has generated various challenges during the modelling process.

We have some ideas about additional actions that could improve the results.

##Changes in the response variable

The field shares could be transformed into 3 different categories: Low_Number_Shares (0), High_Number_Shares (1) and an indetermined zone.

The indetermined zone could be excluded from the database (as it is a grey area and might generate some distortion in the modelling process), whereas the Low_Number_Shares (0) and High_Number_Shares (1) distinction might help to distinguish in a clearer way characteristics that influcence the success of an article (measured in # of shares). The selection of the way to split the table has to respond to a logic process and has to take into account the distribution of the information.

As we would work with a binary response variable, it would be appropriate to work with a logit regression model.

##Changes in the independent variables

It might be necessary to generate additional variables or transformations to increase the predicting capacity of the variables. We didn't perform these tasks in this exercise because of the excessive number of independent variables to analyze (almost 60).